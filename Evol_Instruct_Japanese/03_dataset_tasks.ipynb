{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# HuggingFace Hubからデータセットを取得\n",
    "dataset = load_dataset(\"kunishou/oasst1-89k-ja\", split='train')\n",
    "\n",
    "# データセットをpandas DataFrameに変換\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# DataFrameをJSONファイルとして保存（日本語を正しく扱うために ensure_ascii=False を設定）\n",
    "df.to_json(\"./data/oasst1-89k-ja.jsonl\", force_ascii=False, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oasstデータセットの取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja)\n",
    " - oasstデータセットはChatデータなので、１ターンでは成立しないデータが混じっている。そのため、利用は非推奨。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# oasst1のオリジナルデータのロード\n",
    "ds = load_dataset(\"OpenAssistant/oasst1\")\n",
    "train = ds[\"train\"].to_pandas()\n",
    "val = ds[\"validation\"].to_pandas()\n",
    "\n",
    "df_origin = pd.concat([train, val], axis=0).reset_index(drop=True)\n",
    "\n",
    "# oasst1日本語翻訳データの読み込み\n",
    "df_ja = load_dataset(\"kunishou/oasst1-89k-ja\", split='train').to_pandas()\n",
    "\n",
    "# oasst1のオリジナルデータと日本語翻訳データのマージ\n",
    "df = pd.merge(df_origin, df_ja[[\"message_id\", \"text_ja\"]], on=\"message_id\", how=\"left\").copy()\n",
    "df[\"text\"] = df[\"text_ja\"]\n",
    "\n",
    "df_assistant = df[(df.role == \"assistant\")].copy()\n",
    "df_prompter = df[(df.role == \"prompter\")].copy()\n",
    "df_prompter = df_prompter.set_index(\"message_id\")\n",
    "df_assistant[\"output\"] = df_assistant[\"text\"].values\n",
    "\n",
    "inputs = []\n",
    "parent_ids = []\n",
    "for _, row in df_assistant.iterrows():\n",
    "    input = df_prompter.loc[row.parent_id]\n",
    "    inputs.append(input.text)\n",
    "    parent_ids.append(input.parent_id)\n",
    "\n",
    "df_assistant[\"instruction\"] = inputs\n",
    "df_assistant[\"parent_id\"] = parent_ids\n",
    "\n",
    "df_assistant = df_assistant[\n",
    "    [\"instruction\", \"output\", \"message_id\", \"parent_id\", \"lang\", \"rank\"]\n",
    "].rename(columns={\"message_id\": \"id\"})\n",
    "\n",
    "\n",
    "# 翻訳タスクのみデータに異常があるので除外\n",
    "df_assistant2 = df_assistant[~df_assistant[\"instruction\"].str.contains(\"翻訳\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これ以下でjsonファイルへ書き出し---------------\n",
    "\n",
    "learn_datas = []\n",
    "input_list = []\n",
    "\n",
    "for n in range(len(df_assistant2)):\n",
    "    learn_data = {\n",
    "        \"id\": str(df_assistant2.iloc[n, 2]),  # \"id\"カラムの追加\n",
    "        \"instruction\": str(df_assistant2.iloc[n, 0]),\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\n",
    "    }\n",
    "\n",
    "    input_list.append(df_assistant2.iloc[n, 0])\n",
    "    learn_data[\"input\"] = \"\"\n",
    "    learn_data[\"output\"] = str(df_assistant2.iloc[n, 1])\n",
    "\n",
    "    learn_datas.append(learn_data)\n",
    "\n",
    "json_learn_data = json.dumps(learn_datas, indent=4, ensure_ascii=False)\n",
    "with open('./data/oasst1-89k-ja.json', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(json_learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpacaのデータセットの形式変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSONLファイルを読み込む\n",
    "with open('./data/seed_tasks_jp_cleaned.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# フォーマットを変更\n",
    "updated_data = []\n",
    "for item in data:\n",
    "    if 'instances' in item:\n",
    "        for instance in item['instances']:\n",
    "            new_item = {\n",
    "                'id': item['id'],\n",
    "                'name': item['name'],\n",
    "                'instruction': item['instruction'],\n",
    "                'input': instance['input'],\n",
    "                'output': instance['output'],\n",
    "                'is_classification': item['is_classification']\n",
    "            }\n",
    "            updated_data.append(new_item)\n",
    "    else:\n",
    "        updated_data.append(item)\n",
    "\n",
    "# 更新されたデータをJSONLファイルに書き込む\n",
    "with open('./data/alpaca_seed_tasks_jp.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for item in updated_data:\n",
    "        json.dump(item, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hh-rlhf-49k-jaデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"kunishou/hh-rlhf-49k-ja\")\n",
    "dataset.set_format(type=\"pandas\")\n",
    "df = dataset[\"train\"][:]\n",
    "df = df[df[\"ng_translation\"]!=\"1\"].drop([\"ng_translation\", \"index\"], axis=1).reset_index()\n",
    "df = df.rename(columns={\"index\": \"id\"})\n",
    "df = df[[\"id\", \"instruction\", \"input\", \"output\"]]\n",
    "\n",
    "# DataFrameをJSONファイルとして保存（日本語を正しく扱うために ensure_ascii=False を設定）\n",
    "df.to_json(\"./data/hh-rlhf-49k-ja.jsonl\", force_ascii=False, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yagiharahajime/Documents/06_プロジェクト/29_wizardLM/WizardLM/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c240d6596e8d4dfb8fbbb7a16ed9676b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837840700e96445a8c7bef85958b4507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507ffb23580c4420bacfb3b75c902cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdab66efb7942e0bef6fcb0e97e70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_PATH = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\\\n</llm-code>',\n",
       " [414, 28711, 700, 584, 28719, 28733, 1409, 28767],\n",
       " '</llm-code>')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=\"</llm-code>\"\n",
    "a=\"\\\\n</llm-code>\"\n",
    "tok=tokenizer.encode(a, add_special_tokens=False)\n",
    "back=tokenizer.decode([1867, 584, 28719, 28733, 1409, 28767])\n",
    "a,tok,back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "29_wizardLM",
   "language": "python",
   "name": "29_wizardlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
