{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(data, model, tokenizer):\n",
    "    \"\"\"\n",
    "    データセットからテキストを抽出し、トークナイザーとモデルを使用してテキストのベクトルを生成します。\n",
    "\n",
    "    Args:\n",
    "    data (dict): キーとテキストのリストを含む辞書。\n",
    "    model (transformers.PreTrainedModel): テキストをベクトルに変換するためのモデル。\n",
    "    tokenizer (transformers.PreTrainedTokenizer): テキストをトークンに変換するためのトークナイザー。\n",
    "\n",
    "    Returns:\n",
    "    dict: 各キーに対応するテキストベクトルの辞書。\n",
    "    \"\"\"\n",
    "    text_vectors = {}\n",
    "    for key in data.keys():\n",
    "        texts = [entry['instruction'] for entry in data[key]]\n",
    "        encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        # Get the mean of the last hidden states to use as the sentence vector\n",
    "        sentence_vectors = model_output.last_hidden_state.mean(dim=1).numpy()\n",
    "        text_vectors[key] = sentence_vectors\n",
    "    return text_vectors\n",
    "\n",
    "def plot_dimension_reduction(text_vectors, data, method='tsne'):\n",
    "    \"\"\"\n",
    "    t-SNEまたはPCAを使用して、各キーごとにテキストベクトルを2次元空間に別々のプロットとして表示します。\n",
    "\n",
    "    Args:\n",
    "    text_vectors (dict): テキストベクトルの辞書。\n",
    "    data (dict): 元のデータセット。\n",
    "    method (str): 使用する次元削減の方法 ('tsne' または 'pca')。\n",
    "    \"\"\"\n",
    "    if method == 'tsne':\n",
    "        # perplexityの値をサンプル数に応じて調整\n",
    "        min_samples = min(len(vectors) for vectors in text_vectors.values())\n",
    "        perplexity_value = min(10, max(5, min_samples - 1))  # 5と10の間で、かつサンプル数-1以下\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "    elif method == 'pca':\n",
    "        from sklearn.decomposition import PCA\n",
    "        reducer = PCA(n_components=2)\n",
    "    else:\n",
    "        raise ValueError(\"無効な次元削減方法が指定されました。'tsne' または 'pca' を選択してください。\")\n",
    "\n",
    "    colors = plt.cm.get_cmap('tab20', len(data.keys()))\n",
    "\n",
    "    for idx, (key, vectors) in enumerate(text_vectors.items()):\n",
    "        plt.figure()\n",
    "        transformed_vectors = reducer.fit_transform(vectors)\n",
    "        ids = [entry['id'] for entry in data[key]]\n",
    "        unique_ids = list(set(ids))\n",
    "        color_map = {uid: colors(i) for i, uid in enumerate(unique_ids)}\n",
    "        label_added = set()\n",
    "\n",
    "        for i, vec in enumerate(transformed_vectors):\n",
    "            if ids[i] not in label_added:\n",
    "                plt.scatter(*vec, color=color_map[ids[i]], label=ids[i])\n",
    "                label_added.add(ids[i])\n",
    "            else:\n",
    "                plt.scatter(*vec, color=color_map[ids[i]])\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.title(f'{method.upper()} plot of Instructions for {key}')\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./test_output/test_generated_for_check_diversity_v1.1.json\"\n",
    "data = utils.load_json_file(file_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "text_vectors = get_text_vectors(data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"tsne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./test_output/test_generated_for_check_diversity_v1.0.json\"\n",
    "data = utils.load_json_file(file_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "text_vectors = get_text_vectors(data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"tsne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./test_output/test_generated_for_check_diversity_v1.2.json\"\n",
    "data = utils.load_json_file(file_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "text_vectors = get_text_vectors(data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"tsne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./test_output/test_generated_for_check_diversity_v1.3.json\"\n",
    "data = utils.load_json_file(file_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "text_vectors = get_text_vectors(data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dimension_reduction(text_vectors, data, method=\"tsne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "29_wizardLM",
   "language": "python",
   "name": "29_wizardlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
